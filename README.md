# FlinkJobRunner

## ğŸš€ Overview

FlinkJobRunner is a containerized Flask-based job orchestration service that automates the deployment and management of Apache Flink streaming jobs with SHACL (Shapes Constraint Language) validation. It provides a RESTful API for running complex data processing workflows that combine semantic data validation, knowledge graph processing, and distributed stream processing.

## ğŸ—ï¸ Architecture

The application acts as a job runner that:

1. **Receives semantic data files** (RDF knowledge graphs and SHACL constraint files) via pre-signed URLs
2. **Orchestrates multi-step workflows** using Make targets for setup, validation, and deployment
3. **Manages Kubernetes resources** for Flink cluster deployment and database operations
4. **Provides real-time logging** through Server-Sent Events (SSE) for job monitoring
5. **Handles PostgreSQL database operations** for storing processed semantic data

## ğŸ”§ Key Features

- **ğŸŒ RESTful API**: Create, monitor, and manage jobs via HTTP endpoints
- **ğŸ“Š Real-time Monitoring**: Server-Sent Events (SSE) for live job log streaming
- **ğŸ”„ Sequential Workflows**: Support for complex multi-step operations (`setup-and-deploy`)
- **â˜¸ï¸ Kubernetes Native**: Designed for in-cluster execution with proper RBAC
- **ğŸ—„ï¸ Database Integration**: PostgreSQL operations with secure credential management
- **ğŸ›¡ï¸ Security**: Runs as non-root user with minimal required permissions
- **ğŸ“¦ Containerized**: Docker-ready with all required tools pre-installed

## ğŸ› ï¸ Supported Tools & Technologies

The container includes all necessary tools for the complete workflow:

- **Apache Flink**: Stream processing engine deployment
- **Kubernetes**: Cluster management (kubectl, helm, helmfile)
- **PostgreSQL**: Database client (psql) for data operations
- **SHACL Processing**: Python libraries (rdflib, pyshacl) for semantic validation
- **YAML Processing**: yq v3.4.1 for configuration management
- **Build Tools**: Make, Python 3.12, and development dependencies

## ğŸ“‹ Prerequisites

- Docker for building the container image
- Kubernetes cluster with:
  - PostgreSQL database (acid-cluster or similar)
  - Strimzi Kafka operator
  - Flink operator
  - Appropriate RBAC permissions

## ğŸš€ Quick Start

### 1. Build the Container

```bash
# Simple build
./build.sh

# Build with custom name and tag
IMAGE_NAME=my-flink-runner IMAGE_TAG=v1.0.0 ./build.sh

# Build and push to registry
REGISTRY=your-registry.com ./build.sh
```

### 2. Run Locally (Development)

```bash
# With kubeconfig file
docker run -p 8080:8080 \
  -v ~/.kube/config:/app/secrets/kubeconfig \
  flink-job-runner

# Check health
curl http://localhost:8080/healthz
```

### 3. Deploy to Kubernetes

```bash
# Update the image name in k8s-deployment.yaml, then:
kubectl apply -f k8s-deployment.yaml
```

## ğŸ“¡ API Usage

### Create a Job

```bash
POST /jobs
Content-Type: application/json

{
  "jobId": "optional-custom-id",
  "target": "setup-and-deploy",
  "urls": {
    "knowledge": "https://your-bucket/knowledge.ttl",
    "shacl": "https://your-bucket/shacl.ttl"
  },
  "context": {
    "kubeContext": "optional-context"
  }
}
```

### Monitor Job Progress

```bash
# Get job status
GET /jobs/{jobId}

# Stream live logs (Server-Sent Events)
GET /jobs/{jobId}/logs
```

### Cancel Running Job

```bash
POST /jobs/{jobId}/cancel
```

## ğŸ¯ Supported Targets

- **`setup`**: Install dependencies and prepare environment
- **`flink-deploy`**: Deploy Flink cluster and processing jobs
- **`setup-and-deploy`**: Sequential execution of setup + flink-deploy
- **`validate`**: Run SHACL validation only
- **`plan`**: Generate deployment plan without execution

## ğŸ”’ Security & RBAC

The application requires the following Kubernetes permissions:

- **Pods/Services**: CRUD operations for Flink components
- **Deployments**: Scaling and management of Strimzi operators  
- **Secrets**: Access to database credentials
- **ConfigMaps**: Configuration management
- **KafkaTopics**: Strimzi resource management

See `k8s-deployment.yaml` for complete RBAC configuration.

## ğŸ“Š Monitoring & Logging

### Health Check
```bash
curl http://localhost:8080/healthz
```

### Real-time Logs
```javascript
// JavaScript example for SSE log streaming
const eventSource = new EventSource('/jobs/your-job-id/logs');
eventSource.onmessage = function(event) {
  console.log('Log:', event.data);
};
```

### Job Status
```bash
curl http://localhost:8080/jobs/your-job-id
```

## ğŸ³ Container Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `RUNNER_BIND` | `0.0.0.0` | Flask server bind address |
| `RUNNER_PORT` | `8080` | Flask server port |
| `DIGITALTWIN_ROOT` | `/app/work/shacl2flink` | Path to shacl2flink tools |
| `WORK_ROOT` | `/app/work` | Working directory for jobs |
| `KUBECONFIG` | `` | Kubeconfig path (empty for in-cluster) |
| `ALLOWED_TARGETS` | `setup,flink-deploy,...` | Comma-separated allowed targets |

### Volume Mounts

- `/app/work` - Persistent storage for job data (recommended: 10Gi)
- `/var/run/secrets/kubernetes.io/serviceaccount` - Service account token (auto-mounted)

## ğŸ”§ Development

### Local Development Setup

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies  
pip install -r requirements.txt
pip install -r work/shacl2flink/requirements.txt
pip install -r work/shacl2flink/requirements-dev.txt

# Run locally
python3 app.py
```

### Project Structure

```
FlinkJobRunner/
â”œâ”€â”€ app.py                    # Main Flask application
â”œâ”€â”€ alerts_shacl.py          # SHACL alerts blueprint
â”œâ”€â”€ requirements.txt         # Root Python dependencies
â”œâ”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ build.sh               # Build script
â”œâ”€â”€ k8s-deployment.yaml    # Kubernetes deployment
â”œâ”€â”€ work/
â”‚   â”œâ”€â”€ shacl2flink/       # Core processing tools
â”‚   â”‚   â”œâ”€â”€ Makefile       # Build and deployment targets
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ requirements-dev.txt
â”‚   â””â”€â”€ helm/              # Helm charts and configurations
â””â”€â”€ secrets/
    â””â”€â”€ kubeconfig         # Local development kubeconfig
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test locally and in Kubernetes
5. Submit a pull request

## ğŸ“„ License

See LICENSE file for details.

---

**Built for Industry 4.0 semantic data processing workflows** ğŸ­âœ¨